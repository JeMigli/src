{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "inf8460_tp3_A20.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JeMigli/src/blob/master/inf8460_tp3_A20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hV_jrGV5Rbjb"
      },
      "source": [
        "# École Polytechnique de Montréal\n",
        "# Département Génie Informatique et Génie Logiciel\n",
        "\n",
        "# INF8460 – Traitement automatique de la langue naturelle - TP3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUGBwWplRbjc"
      },
      "source": [
        "# Objectifs d’apprentissage\n",
        " • Utiliser des plongements lexicaux pré-entrainés pour de la classification\n",
        " \n",
        " • Entrainer des plongements lexicaux de type word2vec\n",
        " \n",
        " • Implanter des modèles de classification neuronaux"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "lwUz9uWbRbjd"
      },
      "source": [
        "## Équipe et contributions \n",
        "Veuillez indiquer la contribution effective de chaque membre de l'équipe en pourcentage et en indiquant les modules ou questions sur lesquelles chaque membre a travaillé"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "_c0sPCLfRbjd"
      },
      "source": [
        "Vincent Dandenault: 33% (détail)\n",
        "\n",
        "Dominique Piché: 33% (détail)\n",
        "\n",
        "Jérémie Miglierina: 33% (détail)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYnauKfdRbje"
      },
      "source": [
        "# Librairies externes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-24T13:30:14.696418Z",
          "start_time": "2020-09-24T13:30:14.651596Z"
        },
        "id": "DvoqXw1RRbje",
        "outputId": "5a9f0af8-87fa-41f1-9232-8be457a076fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "import gensim\n",
        "import io\n",
        "import nltk\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import requests\n",
        "import sklearn\n",
        "import sklearn.naive_bayes\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from typing import Dict\n",
        "import zipfile\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download(\"wordnet\")\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdTV1nhdRbji"
      },
      "source": [
        "# Téléchargement et lecture des données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-24T13:06:47.074618Z",
          "start_time": "2020-09-24T13:06:47.026757Z"
        },
        "id": "ZqTdcf7gRbji"
      },
      "source": [
        "DATA_PATH = os.path.join(os.getcwd(), \"aclImdb\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "IxCDJgXJRbjl"
      },
      "source": [
        "## Téléchargement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-22T15:19:34.239196Z",
          "start_time": "2020-09-22T15:16:55.591044Z"
        },
        "hidden": true,
        "id": "3GWWQvIRRbjl",
        "outputId": "60f6c32c-0997-4410-a364-ffc897c834bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xzf aclImdb_v1.tar.gz\n",
        "!rm aclImdb_v1.tar.gz\n",
        "!echo Done!"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-18 14:24:35--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  42.5MB/s    in 1.9s    \n",
            "\n",
            "2020-10-18 14:24:37 (42.5 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "lJCQ3aMERbjo"
      },
      "source": [
        "def download_wikipedia_embeddings() -> None:\n",
        "    if not os.path.exists(os.path.join(os.getcwd(), \"model.txt\")):\n",
        "        res = requests.get(\"http://vectors.nlpl.eu/repository/11/3.zip\")\n",
        "        with zipfile.ZipFile(io.BytesIO(res.content)) as z:\n",
        "            z.extractall(\"./\")\n",
        "        os.remove(os.path.join(os.getcwd(), \"3.zip\"))\n",
        "        os.remove(os.path.join(os.getcwd(), \"meta.json\"))\n",
        "        os.remove(os.path.join(os.getcwd(), \"model.bin\"))\n",
        "        os.remove(os.path.join(os.getcwd(), \"README\"))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "J9NUxy-NRbjq"
      },
      "source": [
        "## Lecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-24T13:06:48.847418Z",
          "start_time": "2020-09-24T13:06:48.818869Z"
        },
        "hidden": true,
        "id": "AhZV5v8rRbjr"
      },
      "source": [
        "def read_data(path):\n",
        "    traintest = ['train', 'test']\n",
        "    classes = ['pos', 'neg']\n",
        "    corpus = {cls: [] for cls in classes}\n",
        "\n",
        "    # Each data is a list of strings(reviews)\n",
        "    reviews = []\n",
        "    labels = []\n",
        "    for cls in classes:\n",
        "        dir_path = os.path.join(path, cls)\n",
        "        \n",
        "        for filename in os.listdir(dir_path):\n",
        "            file = os.path.join(dir_path, filename)\n",
        "            with open(file, encoding = 'utf-8') as f:\n",
        "                corpus[cls].append(f.read().replace(\"\\n\", \" \"))\n",
        "        \n",
        "    return corpus"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-24T13:06:51.435025Z",
          "start_time": "2020-09-24T13:06:50.020587Z"
        },
        "hidden": true,
        "id": "WNt335YeRbjt"
      },
      "source": [
        "train_data = read_data(os.path.join(DATA_PATH, 'train'))\n",
        "test_data = read_data(os.path.join(DATA_PATH, 'test'))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-24T13:06:51.480512Z",
          "start_time": "2020-09-24T13:06:51.437150Z"
        },
        "hidden": true,
        "id": "WJRJsFiURbjw"
      },
      "source": [
        "def create_wikipedia_embeddings(word_indices: Dict[str, int], vocab_len: int) -> np.ndarray:\n",
        "    with open(\"./model.txt\", \"r\", encoding=\"UTF-8\") as f:\n",
        "        shape_string = f.readline()\n",
        "        lines = f.readlines() \n",
        "        \n",
        "    embedding = np.zeros((vocab_len, 300), dtype=float)\n",
        "    for line in lines:\n",
        "        splitted_line = line.split(\" \")\n",
        "        word = splitted_line[0].split(\"_\")[0]\n",
        "        if word in word_indices and word_indices[word] < vocab_len:\n",
        "            embedding_line = splitted_line[1:]\n",
        "            embedding[word_indices[word]] = list(map(float, embedding_line))\n",
        "        \n",
        "    return embedding"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "KCQPrLn7Rbjy"
      },
      "source": [
        "## Prétraitement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-24T13:06:54.237924Z",
          "start_time": "2020-09-24T13:06:54.204609Z"
        },
        "hidden": true,
        "id": "Ov9xZjmIRbjy"
      },
      "source": [
        "class Preprocess(object):\n",
        "    def __init__(self, lemmatize=True):\n",
        "        self.stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
        "        self.lemmatize = lemmatize\n",
        "\n",
        "    def preprocess_pipeline(self, data):\n",
        "        clean_tokenized_data = self._clean_doc(data)\n",
        "        if self.lemmatize:\n",
        "            clean_tokenized_data = self._lemmatize(clean_tokenized_data)\n",
        "\n",
        "        return clean_tokenized_data\n",
        "\n",
        "    def _clean_doc(self, data):\n",
        "        tokenizer = nltk.tokenize.RegexpTokenizer(r\"\\w+\")\n",
        "        return [\n",
        "            [\n",
        "                token.lower()\n",
        "                for token in tokenizer.tokenize(review)\n",
        "                if token.lower() not in self.stopwords\n",
        "                and len(token) > 1\n",
        "                and token.isalpha()\n",
        "                and token != \"br]\"\n",
        "            ]\n",
        "            for review in data\n",
        "        ]\n",
        "\n",
        "    def _lemmatize(self, data):\n",
        "        lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "        return [[lemmatizer.lemmatize(word) for word in review] for review in data]\n",
        "\n",
        "    def convert_to_reviews(self, tokenized_reviews):\n",
        "        reviews = []\n",
        "        for tokens in tokenized_reviews:\n",
        "            reviews.append(\" \".join(tokens))\n",
        "\n",
        "        return reviews"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-24T13:07:29.745222Z",
          "start_time": "2020-09-24T13:06:55.097985Z"
        },
        "hidden": true,
        "id": "0YtQCrU8Rbj0",
        "outputId": "72978d8f-fdf7-4feb-bb11-8a25eb188e19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "pre = Preprocess()\n",
        "\n",
        "train_pos = pre.preprocess_pipeline(train_data[\"pos\"])\n",
        "train_neg = pre.preprocess_pipeline(train_data[\"neg\"])\n",
        "test_pos = pre.preprocess_pipeline(test_data[\"pos\"])\n",
        "test_neg = pre.preprocess_pipeline(test_data[\"neg\"])\n",
        "\n",
        "y_train = [1] * len(train_pos) + [0] * len(train_neg)\n",
        "y_test = [1] * len(test_pos) + [0] * len(test_neg)\n",
        "X_train = [\" \".join(sentence) for sentence in train_pos + train_neg]\n",
        "X_test = [\" \".join(sentence) for sentence in test_pos + test_neg]\n",
        "\n",
        "print(\"{} training sentences: {} pos and {} neg\".format(len(X_train), len(train_pos), len(train_neg)))\n",
        "print(\"{} test sentences: {} pos and {} neg\".format(len(X_test), len(test_pos), len(test_neg)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000 training sentences: 12500 pos and 12500 neg\n",
            "25000 test sentences: 12500 pos and 12500 neg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Utt6zgP5Md3",
        "outputId": "1e89ab5d-11a5-417e-ed02-548d6994e729",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(X_test[0])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "commenter stated movie worst ever forced upon child say though loved movie little still love today movie best running theme family togetherness considering time period movie released thought movie acted well wish could still find copy somewhere film watched kid favorite know probably watched least week brother mom would definitely recommend anyone know know find copy suggest watching wonderful heartwarming\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M68m8ASzRbj2"
      },
      "source": [
        "# 1. Entrainement de plongements lexicaux"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDjGVtZ0Rbj3"
      },
      "source": [
        "Vous devez réaliser les étapes suivantes:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk4IXNyXRbj3"
      },
      "source": [
        "## a) Utiliser Gensim pour entrainer un modèle word2vec sur le corpus. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVW5D-SBRbj3"
      },
      "source": [
        "\n",
        "\n",
        "from gensim import utils\n",
        "\n",
        "class MyCorpusXTrain(object):\n",
        "    \"\"\"An interator that yields sentences (lists of str).\"\"\"\n",
        "\n",
        "    def __iter__(self):\n",
        "        for line in X_train:\n",
        "            # assume there's one document per line, tokens separated by whitespace\n",
        "            yield utils.simple_preprocess(line)\n",
        "\n",
        "\n",
        "class MyCorpusXTest(object):\n",
        "    \"\"\"An interator that yields sentences (lists of str).\"\"\"\n",
        "\n",
        "    def __iter__(self):\n",
        "        for line in X_test:\n",
        "            # assume there's one document per line, tokens separated by whitespace\n",
        "            yield utils.simple_preprocess(line)\n",
        "\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOsvmtlZs2P4",
        "outputId": "05080b2c-f501-4650-ba15-ac4853f461e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        }
      },
      "source": [
        "import gensim.models\n",
        "\n",
        "sentences = MyCorpusXTrain()\n",
        "model = gensim.models.Word2Vec(sentences=sentences,size = 256,min_count=3, window=5, workers=4)\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-1b62c6035248>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyCorpusXTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)\u001b[0m\n\u001b[1;32m    765\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbow_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbow_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m             fast_version=FAST_VERSION)\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You can't pass a generator as the sentences argument. Try an iterator.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m             self.train(\n\u001b[1;32m    761\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, sentences, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m    941\u001b[0m             trim_rule=trim_rule, **kwargs)\n\u001b[1;32m    942\u001b[0m         \u001b[0mreport_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'memory'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_retained_words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild_vocab_from_freq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_raw_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mprepare_weights\u001b[0;34m(self, hs, negative, wv, update, vocabulary)\u001b[0m\n\u001b[1;32m   1820\u001b[0m         \u001b[0;31m# set initial input/projection and hidden weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1822\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1823\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1824\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mreset_weights\u001b[0;34m(self, hs, negative, wv)\u001b[0m\n\u001b[1;32m   1837\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m             \u001b[0;31m# construct deterministic seed from word AND seed argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1839\u001b[0;31m             \u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseeded_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1840\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mREAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mseeded_vector\u001b[0;34m(self, seed_string, vector_size)\u001b[0m\n\u001b[1;32m   1828\u001b[0m         \u001b[0;31m# Note: built-in hash() may vary by Python version or even (in Py3.x) per launch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1829\u001b[0m         \u001b[0monce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhashfxn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed_string\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;36m0xffffffff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1830\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0monce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1832\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaGmePRg8JNU"
      },
      "source": [
        "\n",
        "print(model.wv.vectors.shape)\n",
        "print(y_test)\n",
        "print(X_test[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5NIEcULX_w2"
      },
      "source": [
        "# Python3 program to count words \n",
        "# in a given string\n",
        "OUT = 0\n",
        "IN = 1\n",
        "\n",
        "# Returns number of words in string\n",
        "def countWords(string):\n",
        "\tstate = OUT\n",
        "\twc = 0\n",
        "\n",
        "\t# Scan all characters one by one\n",
        "\tfor i in range(len(string)):\n",
        "\n",
        "\t\t# If next character is a separator, \n",
        "\t\t# set the state as OUT\n",
        "\t\tif (string[i] == ' ' or string[i] == '\\n' or\n",
        "\t\t\tstring[i] == '\\t'):\n",
        "\t\t\tstate = OUT\n",
        "\n",
        "\t\t# If next character is not a word \n",
        "\t\t# separator and state is OUT, then \n",
        "\t\t# set the state as IN and increment \n",
        "\t\t# word count\n",
        "\t\telif state == OUT:\n",
        "\t\t\tstate = IN\n",
        "\t\t\twc += 1\n",
        "\n",
        "\t# Return the number of words\n",
        "\treturn wc\n",
        "\n",
        "# Driver Code\n",
        "string = X_train[400]\n",
        "print(\"No. of words : \" + str(countWords(string)))\n",
        "\n",
        "# This code is contributed by BHAVYA JAIN\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D40qRIJeRbj5"
      },
      "source": [
        "## b) Décrire les paramètres du ou des modèles entraînés, leur taille sur disque, le nombre de mots encodés, le temps d'entraînement, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBKGRp2SJ3ez"
      },
      "source": [
        "Taille des vecteurs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FY1s-zBPJ4Ni"
      },
      "source": [
        "model.vector_size "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFYC_IrVzZJy"
      },
      "source": [
        "Taille sur le disque\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQ_nraAt0vy0"
      },
      "source": [
        "size_model = model.estimate_memory()\n",
        "total_size_model = size_model[\"total\"]\n",
        "print(\"Taille en byte: \" + str(total_size_model))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxo6hRwzRbj6"
      },
      "source": [
        "Nombre de mot encodé"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74i4KYyGI8oj"
      },
      "source": [
        "print (str(model.corpus_total_words) + \" mots encodés\") ## Je sais pas vraiment si sa retourne les mots encod/\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoVWAdTlI7zL"
      },
      "source": [
        "Temps d'entrainenement:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBMvLYJMI8A0"
      },
      "source": [
        "\n",
        "print(str(model.total_train_time) + \" secondes\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLdsDinhRbj6"
      },
      "source": [
        "## c) Décrire le cas échéant et de manière précise tout problème que vous avez eu à obtenir votre modèle et les façons de résoudre ces problèmes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv75ZDhvRbj6"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkJHsWJqRbj7"
      },
      "source": [
        "## d) Retrouvez les 5 mots voisins des mots suivants : excellent, terrible"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBvZM58VRbj7"
      },
      "source": [
        "print(\"Les 5 mots voisin de excellent avec leur score de rapprochement:\")\n",
        "result = model.similar_by_word(\"excellent\")\n",
        "for x in range(6):\n",
        "  print(\"{}: {:.4f}\".format(*result[x]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ds7YHXoT6_iZ"
      },
      "source": [
        "print(\"Les 5 mots voisin de terrible avec leur score de rapprochement:\")\n",
        "result = model.wv.similar_by_word(\"terrible\")\n",
        "for x in range(6):\n",
        "  print(\"{}: {:.4f}\".format(*result[x]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "417gi14oDhCr"
      },
      "source": [
        "print(model.wv.syn0)\n",
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OCLd3kbRbj9"
      },
      "source": [
        "# 2. Classification avec des plongements lexicaux"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2M39rAaRbj9"
      },
      "source": [
        "On vous demande d’effectuer de la classification avec les plongements lexicaux obtenus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulN502CkRbj-"
      },
      "source": [
        "## a) En reprenant le code développé dans le TP1 avec Scikitlearn, on vous demande cette fois de tester un modèle Naïve Bayes et de régression logistique avec des n-grammes (n=1,2,3 ensemble). Essayez de voir si une réduction de dimension améliore la classification. Ne fournissez que votre meilleur modèle. Evaluez vos algorithmes selon les métriques d’accuracy générale et de F1 par classe sur l’ensemble de test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vZG8hlGRbj-"
      },
      "source": [
        "# Import pipeline\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Import classifiers\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "# Import CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Import other preprocessing modules\n",
        "#from sklearn.preprocessing import Imputer\n",
        "from sklearn.feature_selection import chi2, SelectKBest\n",
        "\n",
        "# Select 300 best features\n",
        "chi_k = 300\n",
        "\n",
        "# Import functional utilities\n",
        "from sklearn.preprocessing import FunctionTransformer, MaxAbsScaler\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "\n",
        "# Perform preprocessing\n",
        "#get_text_data = FunctionTransformer(lambda x: X_train, validate=False)\n",
        "\n",
        "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
        "# Instantiate pipeline: pl\n",
        "pipe_log = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "            transformer_list = [\n",
        "                ('text_features', Pipeline([\n",
        "                    ('vectorizer', CountVectorizer(ngram_range=(1,3))),\n",
        "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
        "                ]))\n",
        "             ]\n",
        "        )),\n",
        "        ('scale', MaxAbsScaler()),\n",
        "        ('clf', LogisticRegression())\n",
        "    ])\n",
        "\n",
        "pipe_log.fit(X_train, y_train)\n",
        "pipe_log.score(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sO3sqZLdQfP"
      },
      "source": [
        "pipe_bayes = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "            transformer_list = [\n",
        "                ('text_features', Pipeline([\n",
        "                    ('vectorizer', CountVectorizer(ngram_range=(1,3))),\n",
        "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
        "                ]))\n",
        "             ]\n",
        "        )),\n",
        "        ('scale', MaxAbsScaler()),\n",
        "        ('clf', MultinomialNB())\n",
        "    ])\n",
        "\n",
        "pipe_bayes.fit(X_train, y_train)\n",
        "pipe_bayes.score(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk48KfNqRbkA"
      },
      "source": [
        "## b) En utilisant Tensorflow (ou Pytorch), on vous demande de développer un classificateur perceptron multicouches et un bi-LSTM avec les vecteurs d’un modèle word2vec pré-entrainé sur Wikipédia en Anglais (enwiki_upos_skipgram_300_3_2019) disponible à http://vectors.nlpl.eu/repository/11/3.zip. \n",
        "\n",
        "On s’attend à ce que vous effectuiez une moyenne des vecteurs de mots de chaque document pour obtenir un plongement du document.  \n",
        "\n",
        "Evaluez vos algorithmes selon les métriques d’accuracy générale et de F1 par classe sur l’ensemble de test. Pour chacun des modèles, indiquez ses performances et ses spécifications (nombre d’époques, régularisation, optimiseur, nombre de couches, etc.). N’hésitez pas à expérimenter avec différents paramètres. Vous ne devez reporter que votre meilleure expérimentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTqvyML1x2UO"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IE3Hi1dxzt4"
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNx9gBMv_cb_"
      },
      "source": [
        "bi-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxFon3xCazf5"
      },
      "source": [
        "import gensim.models\n",
        "\n",
        "sentences_X_train = MyCorpusXTrain()\n",
        "model_X_train = gensim.models.Word2Vec(sentences=sentences_X_train,size = 256,min_count=3, window=5, workers=4)\n",
        "\n",
        "# sentences_X_test = MyCorpusXTest()\n",
        "# model_X_test = gensim.models.Word2Vec(sentences=sentences_X_test,size = 256,min_count=3, window=5, workers=4)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVdwA5CIRbkA",
        "outputId": "dd2a0db5-fcfe-4d60-e959-a36b9a036102",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        }
      },
      "source": [
        "\n",
        "\n",
        "## Setup\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "max_features = 33058  # Only consider the top 20k words\n",
        "maxlen = 150  # Only consider the first 200 words of each movie review\n",
        "\n",
        "\n",
        "## Build the model\n",
        "\n",
        "\n",
        "# # Input for variable-length sequences of integers\n",
        "# inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
        "# # Embed each integer in a 128-dimensional vector\n",
        "# x = layers.Embedding(max_features, 128)(inputs)\n",
        "# # Add 2 bidirectional LSTMs\n",
        "# x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
        "# x = layers.Bidirectional(layers.LSTM(64))(x)\n",
        "# # Add a classifier\n",
        "# outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "# model = keras.Model(inputs, outputs)\n",
        "# model.summary()\n",
        "\n",
        "\n",
        "model_bi_lstm_word2vec = keras.Sequential(\n",
        "    [\n",
        "        layers.Embedding(max_features, 128),\n",
        "        layers.Bidirectional(layers.LSTM(64, return_sequences=True)),\n",
        "        layers.Bidirectional(layers.LSTM(64)),\n",
        "      layers.Dense(1, activation=\"sigmoid\")\n",
        "     \n",
        "    ]\n",
        ")\n",
        "model_bi_lstm_word2vec.summary()\n",
        "\n",
        "\n",
        "word_index_vocab = {key:indx for indx,key in enumerate(list(model_X_train.wv.vocab.keys()))}\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.word_index = word_index_vocab\n",
        "X_sequence =  tokenizer.texts_to_sequences(X_train)\n",
        "X_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "print(type(X_sequence[0]))\n",
        "\n",
        "##print(len(X_val), \"Validation sequences\")\n",
        "# Use pad_sequence to standardize sequence length:\n",
        "# this will truncate sequences longer than 200 words and zero-pad sequences shorter than 200 words.\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(X_sequence, maxlen=maxlen)\n",
        "x_val = keras.preprocessing.sequence.pad_sequences(X_sequences, maxlen=maxlen)\n",
        "\n",
        "\n",
        "\n",
        "## Train and evaluate the model\n",
        "\n",
        "model_bi_lstm_word2vec.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\",f1_m, precision_m, recall_m])\n",
        "model_bi_lstm_word2vec.fit(x_train, np.array(y_train), batch_size=32, epochs=10, validation_split=0.15)\n",
        "metrics_bi_lstm_word2vec = model_bi_lstm_word2vec.evaluate(x_val, np.array(y_test), verbose=0)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_28\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_23 (Embedding)     (None, None, 128)         4231424   \n",
            "_________________________________________________________________\n",
            "bidirectional_24 (Bidirectio (None, None, 128)         98816     \n",
            "_________________________________________________________________\n",
            "bidirectional_25 (Bidirectio (None, 128)               98816     \n",
            "_________________________________________________________________\n",
            "dense_69 (Dense)             (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 4,429,185\n",
            "Trainable params: 4,429,185\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "<class 'list'>\n",
            "Epoch 1/10\n",
            "665/665 [==============================] - 265s 399ms/step - loss: 0.3706 - accuracy: 0.8380 - f1_m: 0.8663 - precision_m: 0.8540 - recall_m: 0.8955 - val_loss: 0.5806 - val_accuracy: 0.8240 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 2/10\n",
            "665/665 [==============================] - 262s 394ms/step - loss: 0.1730 - accuracy: 0.9362 - f1_m: 0.9431 - precision_m: 0.9451 - recall_m: 0.9447 - val_loss: 0.4361 - val_accuracy: 0.8304 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 3/10\n",
            "665/665 [==============================] - 266s 399ms/step - loss: 0.0816 - accuracy: 0.9725 - f1_m: 0.9759 - precision_m: 0.9758 - recall_m: 0.9776 - val_loss: 0.5823 - val_accuracy: 0.8285 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 4/10\n",
            "665/665 [==============================] - 265s 399ms/step - loss: 0.0735 - accuracy: 0.9733 - f1_m: 0.9751 - precision_m: 0.9757 - recall_m: 0.9766 - val_loss: 0.6181 - val_accuracy: 0.8149 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 5/10\n",
            "665/665 [==============================] - 263s 395ms/step - loss: 0.0554 - accuracy: 0.9823 - f1_m: 0.9831 - precision_m: 0.9821 - recall_m: 0.9854 - val_loss: 0.6712 - val_accuracy: 0.8328 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 6/10\n",
            "665/665 [==============================] - 266s 400ms/step - loss: 0.0251 - accuracy: 0.9921 - f1_m: 0.9926 - precision_m: 0.9925 - recall_m: 0.9933 - val_loss: 0.5258 - val_accuracy: 0.8832 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 7/10\n",
            "665/665 [==============================] - 264s 397ms/step - loss: 0.0264 - accuracy: 0.9910 - f1_m: 0.9922 - precision_m: 0.9929 - recall_m: 0.9920 - val_loss: 0.9410 - val_accuracy: 0.8117 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 8/10\n",
            "665/665 [==============================] - 266s 401ms/step - loss: 0.0171 - accuracy: 0.9951 - f1_m: 0.9940 - precision_m: 0.9946 - recall_m: 0.9938 - val_loss: 0.8220 - val_accuracy: 0.8240 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 9/10\n",
            "665/665 [==============================] - 266s 399ms/step - loss: 0.0184 - accuracy: 0.9941 - f1_m: 0.9950 - precision_m: 0.9950 - recall_m: 0.9952 - val_loss: 0.8764 - val_accuracy: 0.7957 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 10/10\n",
            "665/665 [==============================] - 271s 408ms/step - loss: 0.0143 - accuracy: 0.9956 - f1_m: 0.9961 - precision_m: 0.9966 - recall_m: 0.9959 - val_loss: 1.1379 - val_accuracy: 0.7747 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8gSid4gDKd7"
      },
      "source": [
        "MLP\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jya6k5aHDKv1",
        "outputId": "d35ce54d-ed53-4379-cdd2-db1a67ef6176",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        }
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dense\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "max_features = 33058  # Only consider the top 20k words\n",
        "maxlen = 150  # Only consider the first 200 words of each movie review\n",
        "\n",
        "\n",
        "\n",
        "model_MLP_word2vec = keras.Sequential(\n",
        "    [\n",
        "        layers.Embedding(max_features, 128),\n",
        "        layers.Dense(16, activation='relu'),\n",
        "        layers.Dense(8, activation='relu'),\n",
        "        layers.Dense(1, activation=\"sigmoid\")\n",
        "     \n",
        "    ]\n",
        ")\n",
        "model_MLP_word2vec.summary()\n",
        "\n",
        "word_index_vocab = {key:indx for indx,key in enumerate(list(model_X_train.wv.vocab.keys()))}\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.word_index = word_index_vocab\n",
        "X_sequence =  tokenizer.texts_to_sequences(X_train)\n",
        "X_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "print(type(X_sequence[0]))\n",
        "\n",
        "##print(len(X_val), \"Validation sequences\")\n",
        "# Use pad_sequence to standardize sequence length:\n",
        "# this will truncate sequences longer than 200 words and zero-pad sequences shorter than 200 words.\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(X_sequence, maxlen=maxlen)\n",
        "x_val = keras.preprocessing.sequence.pad_sequences(X_sequences, maxlen=maxlen)\n",
        "\n",
        "\n",
        "\n",
        "## Train and evaluate the model\n",
        "\n",
        "model_MLP_word2vec.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\",f1_m, precision_m, recall_m])\n",
        "model_MLP_word2vec.fit(x_train, np.array(y_train), batch_size=32, epochs=10, validation_data=(x_val, np.array(y_test)))\n",
        "metrics_MLP_word2vec = model_MLP_word2vec.evaluate(x_val, np.array(y_test), verbose=0)\n"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_29\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_24 (Embedding)     (None, None, 128)         4231424   \n",
            "_________________________________________________________________\n",
            "dense_70 (Dense)             (None, None, 16)          2064      \n",
            "_________________________________________________________________\n",
            "dense_71 (Dense)             (None, None, 8)           136       \n",
            "_________________________________________________________________\n",
            "dense_72 (Dense)             (None, None, 1)           9         \n",
            "=================================================================\n",
            "Total params: 4,233,633\n",
            "Trainable params: 4,233,633\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "<class 'list'>\n",
            "Epoch 1/10\n",
            "782/782 [==============================] - 46s 59ms/step - loss: 0.6810 - accuracy: 0.5421 - f1_m: 1.0819 - precision_m: 0.5539 - recall_m: 81.8298 - val_loss: 0.6807 - val_accuracy: 0.5481 - val_f1_m: 0.9902 - val_precision_m: 0.4995 - val_recall_m: 55.9270\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 45s 57ms/step - loss: 0.6728 - accuracy: 0.5596 - f1_m: 1.1168 - precision_m: 0.5626 - recall_m: 87.9156 - val_loss: 0.6822 - val_accuracy: 0.5486 - val_f1_m: 0.9904 - val_precision_m: 0.4995 - val_recall_m: 56.8745\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 45s 57ms/step - loss: 0.6706 - accuracy: 0.5596 - f1_m: 1.1204 - precision_m: 0.5646 - recall_m: 87.7844 - val_loss: 0.6834 - val_accuracy: 0.5491 - val_f1_m: 0.9901 - val_precision_m: 0.4995 - val_recall_m: 54.9358\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 45s 58ms/step - loss: 0.6696 - accuracy: 0.5617 - f1_m: 1.1171 - precision_m: 0.5627 - recall_m: 92.4388 - val_loss: 0.6836 - val_accuracy: 0.5487 - val_f1_m: 0.9899 - val_precision_m: 0.4995 - val_recall_m: 54.0438\n",
            "Epoch 5/10\n",
            "782/782 [==============================] - 46s 58ms/step - loss: 0.6691 - accuracy: 0.5616 - f1_m: 1.1287 - precision_m: 0.5689 - recall_m: 85.9251 - val_loss: 0.6848 - val_accuracy: 0.5489 - val_f1_m: 0.9901 - val_precision_m: 0.4995 - val_recall_m: 55.2877\n",
            "Epoch 6/10\n",
            "782/782 [==============================] - 45s 58ms/step - loss: 0.6689 - accuracy: 0.5593 - f1_m: 1.1132 - precision_m: 0.5605 - recall_m: 94.0938 - val_loss: 0.6854 - val_accuracy: 0.5441 - val_f1_m: 0.9806 - val_precision_m: 0.4996 - val_recall_m: 26.4254\n",
            "Epoch 7/10\n",
            "782/782 [==============================] - 44s 57ms/step - loss: 0.6686 - accuracy: 0.5584 - f1_m: 1.1184 - precision_m: 0.5636 - recall_m: 88.6890 - val_loss: 0.6852 - val_accuracy: 0.5489 - val_f1_m: 0.9901 - val_precision_m: 0.4995 - val_recall_m: 55.1055\n",
            "Epoch 8/10\n",
            "782/782 [==============================] - 44s 56ms/step - loss: 0.6683 - accuracy: 0.5609 - f1_m: 1.1345 - precision_m: 0.5719 - recall_m: 84.6030 - val_loss: 0.6851 - val_accuracy: 0.5490 - val_f1_m: 0.9899 - val_precision_m: 0.4995 - val_recall_m: 54.1665\n",
            "Epoch 9/10\n",
            "782/782 [==============================] - 43s 55ms/step - loss: 0.6681 - accuracy: 0.5616 - f1_m: 1.1279 - precision_m: 0.5685 - recall_m: 84.3644 - val_loss: 0.6854 - val_accuracy: 0.5441 - val_f1_m: 0.9801 - val_precision_m: 0.4996 - val_recall_m: 25.7601\n",
            "Epoch 10/10\n",
            "782/782 [==============================] - 46s 59ms/step - loss: 0.6680 - accuracy: 0.5609 - f1_m: 1.1211 - precision_m: 0.5648 - recall_m: 88.7990 - val_loss: 0.6861 - val_accuracy: 0.5438 - val_f1_m: 0.9792 - val_precision_m: 0.4996 - val_recall_m: 24.5348\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uES6YtyW1QYr",
        "outputId": "e52345ba-9abe-4a52-de1e-9ea07eb1b599",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(accuracy_MLP_word2vec)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.6810118556022644, 0.5432206392288208, 0.9805772304534912, 0.4996356666088104, 26.38811492919922]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JUwcVxrRbkD"
      },
      "source": [
        "## c) Ré-entrainez les modèles en b) avec vos propres vecteurs. Comparez maintenant la performance obtenue en en b) avec celles que vous obtenez en utilisant vos propres vecteurs de mots entrainés sur le corpus. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJSQnMX4RbkD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8CmrpVkRbkF"
      },
      "source": [
        "## d) Générez une table ou un graphique qui regroupe les performances des modèles, leurs spécifications, la durée d’entraînement et commentez ces résultats. Quelle est l’influence des word embeddings sur les performances?  Quel est votre meilleur modèle ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Syjz7zxIWBp0",
        "outputId": "c42538cc-cb41-464e-e715-214835a03a69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        }
      },
      "source": [
        "from prettytable import PrettyTable\n",
        "    \n",
        "x = PrettyTable()\n",
        "\n",
        "x.field_names = [\"Model\",\"Accuracy\", \"F1\",\"Recall\",\"Precision\", \"# epoch\",\" #layers\", \"optimizer\", \"loss\", \"loss function\"]\n",
        "\n",
        "x.add_row([\"Bayes\", \"\",\"\",\"\" ,\"\" ,10,\"\",\"\",\"\",\"\"])\n",
        "x.add_row([\"Bi-LSTM with Word2Vec\", metrics_bi_lstm_word2vec[1],metrics_bi_lstm_word2vec[2], metrics_bi_lstm_word2vec[4], metrics_bi_lstm_word2vec[3],10,\"4\",\"Adam\",metrics_bi_lstm_word2vec[0],\"Binary Crossentropy\"])\n",
        "x.add_row([\"Multi-Layer Perceptron with Word2Vec\", metrics_MLP_word2vec[1],metrics_MLP_word2vec[2], metrics_MLP_word2vec[4], metrics_MLP_word2vec[3],10,\"4\",\"Adam\",metrics_MLP_word2vec[0],\"Binary Crossentropy\"])\n",
        "x.add_row([\"Bi-LSTM with our vectors \", \"\",\"\",\"\" ,\"\" ,10,\"\",\"\",\"\",\"\"])\n",
        "x.add_row([\"Multi-Layer Perceptron with our vectors\", \"\",\"\",\"\" ,\"\" ,10,\"\",\"\",\"\",\"\"])\n",
        "\n",
        "\n",
        "\n",
        "print(x)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------------------------------+--------------------+--------------------+---------------------+--------------------+---------+----------+-----------+--------------------+---------------------+\n",
            "|                  Model                  |      Accuracy      |         F1         |        Recall       |     Precision      | # epoch |  #layers | optimizer |        loss        |    loss function    |\n",
            "+-----------------------------------------+--------------------+--------------------+---------------------+--------------------+---------+----------+-----------+--------------------+---------------------+\n",
            "|                  Bayes                  |                    |                    |                     |                    |    10   |          |           |                    |                     |\n",
            "|          Bi-LSTM with Word2Vec          | 0.818880021572113  | 0.467944473028183  | 0.44077685475349426 | 0.4998837411403656 |    10   |    4     |    Adam   | 0.8546251058578491 | Binary Crossentropy |\n",
            "|   Multi-Layer Perceptron with Word2Vec  | 0.5438445806503296 | 0.9792144298553467 |  24.534759521484375 | 0.4996475279331207 |    10   |    4     |    Adam   | 0.6860795021057129 | Binary Crossentropy |\n",
            "|        Bi-LSTM with our vectors         |                    |                    |                     |                    |    10   |          |           |                    |                     |\n",
            "| Multi-Layer Perceptron with our vectors |                    |                    |                     |                    |    10   |          |           |                    |                     |\n",
            "+-----------------------------------------+--------------------+--------------------+---------------------+--------------------+---------+----------+-----------+--------------------+---------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}